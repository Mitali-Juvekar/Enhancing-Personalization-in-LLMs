{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6501c5-81b2-4a90-85bb-ed72af91ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import json\n",
    "\n",
    "file_path = 'tweet.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    dataset = json.load(file)\n",
    "    \n",
    "# Load pre-trained XLNet tokenizer and model\n",
    "# tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "# model = XLNetModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b257f00c-9dc3-40a9-931c-fc03c84a2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1132d1-aa94-409d-b890-7139e3a54f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>input</th>\n",
       "      <th>profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': 'SARS .. H1N1 .. Air France ..  plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>601</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': '@kAtrinaDaniels never thought that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>602</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': '@mattmaloney I feel cheered up. Wow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': 'Night all. Watched Mamma Mia. Did I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>604</td>\n",
       "      <td>Paraphrase the following tweet without any exp...</td>\n",
       "      <td>[{'text': '@hoffifer working, as usual .. Awes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              input  \\\n",
       "0  600  Paraphrase the following tweet without any exp...   \n",
       "1  601  Paraphrase the following tweet without any exp...   \n",
       "2  602  Paraphrase the following tweet without any exp...   \n",
       "3  603  Paraphrase the following tweet without any exp...   \n",
       "4  604  Paraphrase the following tweet without any exp...   \n",
       "\n",
       "                                             profile  \n",
       "0  [{'text': 'SARS .. H1N1 .. Air France ..  plea...  \n",
       "1  [{'text': '@kAtrinaDaniels never thought that ...  \n",
       "2  [{'text': '@mattmaloney I feel cheered up. Wow...  \n",
       "3  [{'text': 'Night all. Watched Mamma Mia. Did I...  \n",
       "4  [{'text': '@hoffifer working, as usual .. Awes...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame.from_dict(dataset)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76541a41-cf79-43f8-945c-f8314abe6228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm currently enjoying the album \"Listen to Eason Chan.\"\n"
     ]
    }
   ],
   "source": [
    "query_start = 'Paraphrase the following tweet without any explanation before or after it: '\n",
    "query = dataset[0]['input'][len(query_start):]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f890793d-0348-4c2e-a28d-56ff5aa4f7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for doc in dataset[0]['profile']:\n",
    "    documents.append(doc['text'])\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801f95be-82ae-4e4e-a792-0e3f5beddca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8baed9-19ac-4821-a4dd-a47de2fb5873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishanalawade/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rephrased Version 1: The album \"Listen to Eason Chan\" is currently in my favorites list.\n",
      "-------------------------------------------\n",
      "Rephrased Version 2: I'm currently enjoying \"Listen to Eason Chan.\"\n",
      "-------------------------------------------\n",
      "Rephrased Version 3: My current listening pleasure is the album \"Listen to Eason Chan.\"\n",
      "-------------------------------------------\n",
      "Rephrased Version 4: \"Listen to Eason Chan\" is the one album I'm currently loving.\n",
      "-------------------------------------------\n",
      "Rephrased Version 5: At the moment, I am enjoying my new album \"Listen to Eason Chan.\"\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def paraphrase_t5(\n",
    "    input_sentence,\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    num_return_sequences=5,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=500\n",
    "):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {input_sentence}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    rephrased_versions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return rephrased_versions\n",
    "\n",
    "# Example usage\n",
    "input_sentence = query\n",
    "rephrased_versions = paraphrase_t5(input_sentence)\n",
    "\n",
    "\n",
    "for i, rephrased_sentence in enumerate(rephrased_versions, start=1):\n",
    "    print(f\"Rephrased Version {i}: {rephrased_sentence}\")\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02479960-083a-4e9f-a73b-97d2ab9ef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained XLNet tokenizer and model\n",
    "tokenizer_xlnet = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model_xlnet = XLNetModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ceac813-bb43-4d2f-b443-2265d5fa1a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "user_profile = dataset[0][\"profile\"]\n",
    "print(len(user_profile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f7f833a-0d15-4e6d-8eec-01604f2b3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embedding = []\n",
    "# Process each document in the first user profile\n",
    "for profile in user_profile:\n",
    "    # Tokenize and extract embeddings for each document\n",
    "    tokenized_document = tokenizer_xlnet(profile[\"text\"], return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        document_outputs = model_xlnet(**tokenized_document)\n",
    "    # Use the last layer output as the document embedding\n",
    "    document_embedding.append(document_outputs.last_hidden_state.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81cdda0c-ba22-4889-bd17-e1dbcd15b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store relevant documents and scores for each version\n",
    "all_relevant_documents = []\n",
    "all_relevant_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c2e23a-7a7b-45e7-9f67-ba0212df4114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Input Version 1: I'm currently enjoying the album \"Listen to Eason Chan.\"\n",
      "==================================================\n",
      "Original Score: tensor([0.9802])\n",
      "Document: listening to eason's 2006 album .. What's going on...? This is my favourite eason album  it's 3.38am\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9796])\n",
      "Document: SARS .. H1N1 .. Air France ..  please cherish your life, people ..\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9796])\n",
      "Document: i am at interchange .. Just missed the bus \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9794])\n",
      "Document: it's friday !! And i just got on the bus .. Going to work later today again \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9794])\n",
      "Document: @waxyx informatics , do u know that? (via @waxyx)no I meant which school haha  I am in ntu\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9780])\n",
      "Document: &quot;See ... You make the world go weird ...&quot; from weiwei's SMS \n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 2: The album \"Listen to Eason Chan\" is currently in my favorites list.\n",
      "==================================================\n",
      "Original Score: tensor([0.9777])\n",
      "Document: &quot;See ... You make the world go weird ...&quot; from weiwei's SMS \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9765])\n",
      "Document: @waxyx informatics , do u know that? (via @waxyx)no I meant which school haha  I am in ntu\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9763])\n",
      "Document: i am at interchange .. Just missed the bus \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9757])\n",
      "Document: SARS .. H1N1 .. Air France ..  please cherish your life, people ..\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9752])\n",
      "Document: listening to eason's 2006 album .. What's going on...? This is my favourite eason album  it's 3.38am\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9751])\n",
      "Document: Finished blogging .. continue to rate restaurants on Facebook .. I wanna get the trophy after rating 100 restaurants \n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 3: I'm currently enjoying \"Listen to Eason Chan.\"\n",
      "==================================================\n",
      "Original Score: tensor([0.9821])\n",
      "Document: i am at interchange .. Just missed the bus \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9813])\n",
      "Document: it's friday !! And i just got on the bus .. Going to work later today again \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9813])\n",
      "Document: @waxyx informatics , do u know that? (via @waxyx)no I meant which school haha  I am in ntu\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9810])\n",
      "Document: SARS .. H1N1 .. Air France ..  please cherish your life, people ..\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9788])\n",
      "Document: &quot;See ... You make the world go weird ...&quot; from weiwei's SMS \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9786])\n",
      "Document: @waxyx I don't know .. I wanted to restart it .. I switch it off and it won't turn on again \n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 4: My current listening pleasure is the album \"Listen to Eason Chan.\"\n",
      "==================================================\n",
      "Original Score: tensor([0.9750])\n",
      "Document: listening to eason's 2006 album .. What's going on...? This is my favourite eason album  it's 3.38am\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9717])\n",
      "Document: &quot;See ... You make the world go weird ...&quot; from weiwei's SMS \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9704])\n",
      "Document: i am at interchange .. Just missed the bus \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9695])\n",
      "Document: @waxyx informatics , do u know that? (via @waxyx)no I meant which school haha  I am in ntu\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9693])\n",
      "Document: SARS .. H1N1 .. Air France ..  please cherish your life, people ..\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9688])\n",
      "Document: it's friday !! And i just got on the bus .. Going to work later today again \n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 5: \"Listen to Eason Chan\" is the one album I'm currently loving.\n",
      "==================================================\n",
      "Original Score: tensor([0.9769])\n",
      "Document: @waxyx informatics , do u know that? (via @waxyx)no I meant which school haha  I am in ntu\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9766])\n",
      "Document: &quot;See ... You make the world go weird ...&quot; from weiwei's SMS \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9765])\n",
      "Document: i am at interchange .. Just missed the bus \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9761])\n",
      "Document: listening to eason's 2006 album .. What's going on...? This is my favourite eason album  it's 3.38am\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9754])\n",
      "Document: Finished blogging .. continue to rate restaurants on Facebook .. I wanna get the trophy after rating 100 restaurants \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9753])\n",
      "Document: addicted to twitter. Time to get out of bed. It's monday \n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing Input Version 6: At the moment, I am enjoying my new album \"Listen to Eason Chan.\"\n",
      "==================================================\n",
      "Original Score: tensor([0.9843])\n",
      "Document: SARS .. H1N1 .. Air France ..  please cherish your life, people ..\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9839])\n",
      "Document: it's friday !! And i just got on the bus .. Going to work later today again \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9832])\n",
      "Document: @waxyx informatics , do u know that? (via @waxyx)no I meant which school haha  I am in ntu\n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9825])\n",
      "Document: @waxyx I don't know .. I wanted to restart it .. I switch it off and it won't turn on again \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9824])\n",
      "Document: addicted to twitter. Time to get out of bed. It's monday \n",
      "\n",
      "==================================================\n",
      "Original Score: tensor([0.9822])\n",
      "Document: i am at interchange .. Just missed the bus \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, input_version in enumerate([input_sentence] + rephrased_versions, start=1):\n",
    "    # Print the input version\n",
    "    print(f\"\\nProcessing Input Version {i}: {input_version}\\n{'='*50}\")\n",
    "\n",
    "    # Retrieve relevant documents for the current version\n",
    "    input_documents = []\n",
    "    input_scores = []\n",
    "\n",
    "    # Encode the current query version\n",
    "    tokenized_paraphrase_rob = tokenizer_xlnet(f'paraphrase: {input_version}', return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "            paraphrase_outputs = model_xlnet(**tokenized_paraphrase_rob)\n",
    "    paraphrase_embedding = paraphrase_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Calculate cosine similarity between the query and paraphrase embeddings\n",
    "    # similarity_scores = util.dot_score(paraphrase_embedding, document_embedding)[0].cpu().tolist()\n",
    "    similarity = []\n",
    "    for j in range(len(document_embedding)):\n",
    "        similarity.append(torch.nn.functional.cosine_similarity(document_embedding[j], paraphrase_embedding.squeeze(dim=1)))\n",
    "    doc_score_title_pairs = list(zip([doc['text'] for doc in dataset[0]['profile']],\n",
    "                                     similarity))\n",
    "    \n",
    "    # Sort by decreasing similarity score\n",
    "    doc_score_title_pairs = sorted(doc_score_title_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Calculate the number of documents to retrieve (top 25%)\n",
    "    num_documents_to_retrieve = int(0.25 * len(doc_score_title_pairs))\n",
    "\n",
    "    # Check if the number of documents to retrieve is greater than 15\n",
    "    if num_documents_to_retrieve > 10:\n",
    "        num_documents_to_retrieve = 10\n",
    "\n",
    "    if num_documents_to_retrieve < 5:\n",
    "        num_documents_to_retrieve = 5\n",
    "        \n",
    "    for text, score in doc_score_title_pairs[:num_documents_to_retrieve]:\n",
    "        print(f\"Original Score: {score}\")\n",
    "        print(f\"Document: {text}\\n\")\n",
    "        \n",
    "        # Store relevant document, title, and score for each interpretation\n",
    "        input_documents.append({'text': text})\n",
    "        input_scores.append(score)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    # Store relevant documents and scores for each version\n",
    "    all_relevant_documents.append(input_documents)\n",
    "    all_relevant_scores.append(input_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c324993-4d0e-49c8-acd4-fbfbdd169f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top  Documents :\n",
      "[{'text': \"@waxyx I don't know .. I wanted to restart it .. I switch it off and it won't turn on again \"}, {'text': \"addicted to twitter. Time to get out of bed. It's monday \"}, {'text': \"it's friday !! And i just got on the bus .. Going to work later today again \"}, {'text': 'SARS .. H1N1 .. Air France ..  please cherish your life, people ..'}, {'text': 'i am at interchange .. Just missed the bus '}, {'text': '@waxyx informatics , do u know that? (via @waxyx)no I meant which school haha  I am in ntu'}, {'text': \"listening to eason's 2006 album .. What's going on...? This is my favourite eason album  it's 3.38am\"}, {'text': \"&quot;See ... You make the world go weird ...&quot; from weiwei's SMS \"}, {'text': 'Finished blogging .. continue to rate restaurants on Facebook .. I wanna get the trophy after rating 100 restaurants '}]\n"
     ]
    }
   ],
   "source": [
    "# Calculate average score for each document across different interpretations\n",
    "average_scores = {}\n",
    "for documents, scores in zip(all_relevant_documents, all_relevant_scores):\n",
    "    for doc_dict, score in zip(documents, scores):\n",
    "        if isinstance(doc_dict, dict):  # Check if it's a dictionary\n",
    "            doc_text = doc_dict.get('text', '')  # Use 'get' to provide a default value if 'text' is not present\n",
    "            if doc_text:\n",
    "                if doc_text not in average_scores:\n",
    "                    average_scores[doc_text] = {'text': doc_text, 'scores': []}\n",
    "                average_scores[doc_text]['scores'].append(score)\n",
    "\n",
    "# Calculate average score for each document\n",
    "average_documents = [{'text': details['text'], 'average_score': sum(details['scores']) / len(details['scores'])}for details in average_scores.values()]\n",
    "\n",
    "# Sort documents based on average scores\n",
    "sorted_documents = sorted(average_documents, key=lambda x: x['average_score'], reverse=True)\n",
    "\n",
    "retrieval_docs = []\n",
    "\n",
    "# Output top 10 documents based on average scores\n",
    "print(\"\\nTop  Documents :\")\n",
    "for doc_dict in sorted_documents[:10]:\n",
    "    new_doc_dict = {'text': doc_dict['text']}\n",
    "    retrieval_docs.append(new_doc_dict)\n",
    "print(retrieval_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffc760f3-ba62-4f8b-9621-94f1b2c4e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The text is: @waxyx I don't know .. I wanted to restart it .. I switch it off and it won't turn on again \n",
      "2. The text is: addicted to twitter. Time to get out of bed. It's monday \n",
      "3. The text is: it's friday !! And i just got on the bus .. Going to work later today again \n",
      "4. The text is: SARS .. H1N1 .. Air France ..  please cherish your life, people ..\n",
      "5. The text is: i am at interchange .. Just missed the bus \n",
      "6. The text is: @waxyx informatics , do u know that? (via @waxyx)no I meant which school haha  I am in ntu\n",
      "7. The text is: listening to eason's 2006 album .. What's going on...? This is my favourite eason album  it's 3.38am\n",
      "8. The text is: &quot;See ... You make the world go weird ...&quot; from weiwei's SMS \n",
      "9. The text is: Finished blogging .. continue to rate restaurants on Facebook .. I wanna get the trophy after rating 100 restaurants \n",
      "\n"
     ]
    }
   ],
   "source": [
    "r_d = ''\n",
    "count = 1\n",
    "for i in retrieval_docs:\n",
    "  # print(i)\n",
    "  r_d += str(count) + '. '\n",
    "  r_d += 'The text is: ' + i['text'] + '\\n'\n",
    "  count += 1\n",
    "  # if count > 7:\n",
    "  #   break\n",
    "\n",
    "print(r_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e745b5d-1172-4af6-9c2a-3ea85f207a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(retrieval_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "232a1e50-2ed5-44cc-8062-b9b509ddaf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U -q google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdc14a7b-f693-449c-b9ea-bb713b28c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ba2449d-4320-4209-96a6-dd91b9ea2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key='AIzaSyBVo_JbfzrPBpHbueQtOiRozzyFK1QK8D0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c668d21-eddb-494a-8107-466fa178fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'query' is defined with a meaningful value\n",
    "query = \"\"\"I'm currently enjoying the album \"Listen to Eason Chan.\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "159be4f3-9869-4220-bd83-4d8b1d5b135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(query, relevant_passage):\n",
    "  escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "  prompt = textwrap.dedent(\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.   \n",
    "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. I'm providing you with some sample tweets written by me for 9 texts examples.\n",
    "  Based on the context provided, can you paraphrase the query to capture my writing style. Make sure it looks like its written by me.\n",
    "  QUESTION: '{query}'\n",
    "  PASSAGE: '{relevant_passage}'\n",
    "\n",
    "    ANSWER:\n",
    "  \"\"\").format(query=query, relevant_passage=escaped)\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e43da860-479e-4538-8643-89778d8bfe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful and informative bot that answers questions using text from the reference passage included below.   \n",
      "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. I'm providing you with some sample tweets written by me for 9 texts examples.\n",
      "  Based on the context provided, can you paraphrase the query to capture my writing style. Make sure it looks like its written by me.\n",
      "  QUESTION: 'I'm currently enjoying the album \"Listen to Eason Chan.\" '\n",
      "  PASSAGE: '1. The text is: @waxyx I dont know .. I wanted to restart it .. I switch it off and it wont turn on again  2. The text is: addicted to twitter. Time to get out of bed. Its monday  3. The text is: its friday !! And i just got on the bus .. Going to work later today again  4. The text is: SARS .. H1N1 .. Air France ..  please cherish your life, people .. 5. The text is: i am at interchange .. Just missed the bus  6. The text is: @waxyx informatics , do u know that? (via @waxyx)no I meant which school haha  I am in ntu 7. The text is: listening to easons 2006 album .. Whats going on...? This is my favourite eason album  its 3.38am 8. The text is: &quot;See ... You make the world go weird ...&quot; from weiweis SMS  9. The text is: Finished blogging .. continue to rate restaurants on Facebook .. I wanna get the trophy after rating 100 restaurants  '\n",
      "\n",
      "    ANSWER:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = make_prompt(query, passage)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e696d3d-8c14-47d4-8deb-669bb35d568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "\n",
    "text_model = text_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f28ed187-f187-48ae-ae85-4b506fe6e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.6\n",
    "answer = palm.generate_text(prompt=prompt,\n",
    "                            model=text_model,\n",
    "                            candidate_count=3,\n",
    "                            temperature=temperature,\n",
    "                            max_output_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91baeebd-ab68-44ac-b586-25eb3f23253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: I'm currently enjoying the album \"Listen to Eason Chan.\" \n",
      "\n",
      "Candidate 2: I am currently listening to the album \"Listen to Eason Chan.\"\n",
      "\n",
      "Candidate 3: I'm currently listening to the album \"Listen to Eason Chan.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, candidate in enumerate(answer.candidates):\n",
    "  print(f\"Candidate {i+1}: {candidate['output']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c95c5d-b419-48db-8f65-6cfb112ebc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
